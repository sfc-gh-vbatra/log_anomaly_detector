# ğŸ‰ Snowflake Integration Complete!

Your log anomaly detection system has been successfully integrated with Snowflake!

## âœ… What Was Created

### ğŸ“ Core Files

1. **`snowflake/setup.sql`**
   - Creates database, schema, tables, stages
   - Sets up warehouse and file formats
   - Defines views for analysis summary

2. **`snowflake/snowpark_analyzer.py`** â­
   - Main analyzer with **TF-IDF vectorization**
   - **Isolation Forest** ML model
   - Feature extraction (12 structured + configurable TF-IDF features)
   - Full pipeline for log processing

3. **`snowflake/stored_procedures.sql`**
   - SQL stored procedures for Snowflake
   - Can be called directly from SQL
   - Includes parsing, vectorization, and anomaly detection

4. **`snowflake/streamlit_app.py`**
   - Interactive web dashboard
   - Upload, analyze, and visualize logs
   - Real-time anomaly detection

5. **`snowflake/quick_start.py`** ğŸš€
   - Automated testing script
   - Validates setup and runs sample analysis
   - Perfect for getting started quickly

### ğŸ“š Documentation

- **`SNOWFLAKE_DEPLOYMENT_GUIDE.md`** - Complete deployment instructions
- **`snowflake/README.md`** - Quick reference guide
- **`snowflake_requirements.txt`** - All Python dependencies
- **`.gitignore`** - Protects credentials from being committed

### âš™ï¸ Configuration

- **`snowflake/snowflake_config.json.example`** - Template for your credentials

---

## ğŸ”‘ Key Features

### Text Vectorization with TF-IDF
- Converts log text into **numerical vectors**
- Captures **semantic meaning** of messages
- Configurable features (default: 100)
- Uses unigrams + bigrams
- Filters common and rare terms

### Machine Learning with Isolation Forest
- Detects anomalies without predefined patterns
- Handles high-dimensional data (112+ features)
- Returns anomaly scores and probabilities
- Configurable contamination factor

### Feature Engineering
**12 Structured Features:**
1. `msg_len` - Message length
2. `has_error` - ERROR level flag
3. `has_warning` - WARNING level flag
4. `has_critical` - CRITICAL level flag
5. `has_failure` - Contains "fail" keyword
6. `has_exception` - Contains "exception"
7. `is_unauthorized` - Unauthorized access
8. `is_connection_issue` - Network problems
9. `has_number` - Contains numbers
10. `has_special_chars` - Special character count
11. `message_frequency` - How often message appears
12. `is_rare_message` - Rare message indicator

**+ TF-IDF Features (default: 100)**
- Vectorized text representation
- Total: **112 features** by default

---

## ğŸš€ Quick Start (5 Minutes)

```bash
# 1. Install dependencies
pip install -r snowflake_requirements.txt

# 2. Configure Snowflake
cd snowflake
cp snowflake_config.json.example snowflake_config.json
# Edit with your credentials:
# - account
# - user
# - password
# - role
# - warehouse (will be created if doesn't exist)

# 3. Run setup SQL
snowsql -f setup.sql
# OR copy/paste into Snowflake Web UI

# 4. Test everything
python quick_start.py
# This will:
# - Test connection
# - Check tables
# - Upload sample logs
# - Run analysis with TF-IDF
# - Show results

# 5. Launch UI
streamlit run streamlit_app.py
# Opens at http://localhost:8501
```

---

## ğŸ“Š How It Works

```
Raw Logs (.txt/.log)
        â†“
[Parse Structure]
        â†“
[Extract 12 Features] â†â”€â”€â”€â”€â”€â”€â”
        â†“                     â”‚
[Vectorize with TF-IDF]       â”‚ Combined
        â†“                     â”‚ Feature
[Generate 100+ Features] â”€â”€â”€â”€â”€â”˜ Matrix
        â†“                       (112+ dimensions)
[Standardize/Normalize]
        â†“
[Isolation Forest Model]
        â†“
[Anomaly Scores + Probabilities]
        â†“
[Save to Snowflake Tables]
        â†“
[Visualize in Streamlit]
```

---

## ğŸ’» Usage Examples

### Python API
```python
from snowflake.snowpark import Session
from snowpark_analyzer import SnowparkLogAnalyzer
import json

# Connect
with open('snowflake_config.json', 'r') as f:
    config = json.load(f)
session = Session.builder.configs(config).create()

# Analyze
analyzer = SnowparkLogAnalyzer(session)
results = analyzer.run_full_pipeline(
    contamination=0.1,    # 10% expected anomalies
    max_features=100      # TF-IDF features
)

# View anomalies
print(results[results['is_anomaly'] == True])
```

### SQL Interface
```sql
-- Run analysis
CALL detect_log_anomalies('ALL', 0.1, 100);

-- View results
SELECT * FROM anomaly_results 
WHERE is_anomaly = TRUE 
ORDER BY anomaly_probability DESC;

-- Summary
SELECT * FROM anomaly_summary;
```

### Streamlit UI
```bash
streamlit run streamlit_app.py
```
Then use the web interface to:
- Upload logs
- Configure parameters
- Run analysis
- View results

---

## ğŸ¯ Tuning Recommendations

### For Production Monitoring
```python
contamination = 0.05    # 5% anomalies (strict)
max_features = 100      # Fast processing
```

### For Security Analysis
```python
contamination = 0.15    # 15% anomalies (permissive)
max_features = 200      # More detailed analysis
```

### For Large Log Volumes
```python
contamination = 0.05    # Focus on clear anomalies
max_features = 50       # Faster processing
```

---

## ğŸ“ˆ Database Schema

### Tables Created
- `raw_logs` - Original log entries
- `parsed_logs` - Structured log data
- `log_features` - Extracted features
- `anomaly_results` - Detection results
- `anomaly_runs` - Historical tracking

### Views
- `anomaly_summary` - Aggregated statistics

### Stage
- `log_files_stage` - For file uploads

---

## ğŸ”’ Security Notes

âš ï¸ **IMPORTANT:**
- Never commit `snowflake_config.json` to git
- Already added to `.gitignore`
- Use environment variables in production
- Consider key-pair authentication for production

---

## ğŸ“š Documentation Files

| File | Purpose |
|------|---------|
| `SNOWFLAKE_DEPLOYMENT_GUIDE.md` | Complete deployment instructions |
| `snowflake/README.md` | Quick reference |
| `SNOWFLAKE_INTEGRATION_SUMMARY.md` | This file - overview |

---

## âœ¨ Next Steps

1. âœ… Review `snowflake_config.json.example` and create your config
2. âœ… Run `setup.sql` in Snowflake
3. âœ… Test with `python quick_start.py`
4. âœ… Launch Streamlit: `streamlit run streamlit_app.py`
5. âœ… Upload your own logs
6. âœ… Tune parameters based on results
7. âœ… Read full deployment guide for advanced features

---

## ğŸ¤ Support

**Common Issues:**

**Q: Connection failed?**
- Check credentials in `snowflake_config.json`
- Verify warehouse is running
- Ensure role has necessary privileges

**Q: No anomalies detected?**
- Increase `contamination` parameter
- Check logs were uploaded: `SELECT COUNT(*) FROM raw_logs`
- Verify sufficient log variety

**Q: Too many anomalies?**
- Decrease `contamination` parameter
- Review log quality
- Adjust TF-IDF parameters

**Q: Slow processing?**
- Reduce `max_features`
- Process files in batches
- Increase Snowflake warehouse size

---

## ğŸŠ You're All Set!

Your log anomaly detection system is now ready to scale with Snowflake!

**Key Capabilities:**
- âœ… TF-IDF text vectorization
- âœ… Isolation Forest ML model
- âœ… 112+ dimensional feature space
- âœ… Snowpark Python integration
- âœ… Interactive Streamlit dashboard
- âœ… SQL stored procedures
- âœ… Production-ready architecture

**Happy anomaly hunting! ğŸ”**

